{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from gensim.utils import tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "from constants import *\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>tfidf-auto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nasa, in preparation for a spacewalk on saturd...</td>\n",
       "      <td>science</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>if professional pride and strong defiance can ...</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>admittedly, the language is reconstructed and ...</td>\n",
       "      <td>arts</td>\n",
       "      <td>arts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>palermo, sicily — roberta vinci beat top-seede...</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the argentine soccer club san lorenzo complete...</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence    label tfidf-auto\n",
       "0  nasa, in preparation for a spacewalk on saturd...  science    science\n",
       "1  if professional pride and strong defiance can ...   sports     sports\n",
       "2  admittedly, the language is reconstructed and ...     arts       arts\n",
       "3  palermo, sicily — roberta vinci beat top-seede...   sports     sports\n",
       "4  the argentine soccer club san lorenzo complete...   sports     sports"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set = NYT\n",
    "df_path = get_data_path(set)\n",
    "df = get_data(set)\n",
    "seeds = get_data(set,type='seedwords')\n",
    "seeds_flat = []\n",
    "for seed in seeds.values():\n",
    "    seeds_flat.extend(seed)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.607252385118526, 0.8480090222954801)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f1_scores(df,y_pred, y_true='label'):\n",
    "    \"\"\"Calculate macro and micro F1 scores.\"\"\"\n",
    "    return (f1_score(df[y_true],df[y_pred],average='macro'),\n",
    "            f1_score(df[y_true],df[y_pred],average='micro'))\n",
    "f1_scores(df,'tfidf-auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings (set,gran='coarse', tokenizer=tokenize):\n",
    "    save_path = os.path.join(MODELS_PATH,set+'.txt')\n",
    "    if not os.path.isfile(save_path): # no saved embeddings, calculate new.\n",
    "        df = get_data(set, granularity=gran)\n",
    "        sents = df['sentence'].apply(tokenizer).apply(tuple)\n",
    "        w2v = Word2Vec(sents)\n",
    "        w2v.wv.save_word2vec_format(save_path)\n",
    "    return KeyedVectors.load_word2vec_format(save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "class TQDMCallback(CallbackAny2Vec):\n",
    "    \"\"\"Callback to visualize Word2Vec training progress using tqdm.\"\"\"\n",
    "    def __init__(self, epochs):\n",
    "        self.progress_bar = tqdm(total=epochs, desc=\"Epochs\", position=0)\n",
    "    def on_epoch_end(self, model):\n",
    "        self.progress_bar.update(1)\n",
    "    def on_train_end(self, model):\n",
    "        self.progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11527/11527 [00:00<00:00, 722362.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting tokenized data to tuples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 11527/11527 [00:08<00:00, 1395.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Word2Vec embeddings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/5 [00:45<?, ?it/s]\n",
      "Epochs:  20%|██        | 1/5 [00:06<00:27,  6.82s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\eviye\\Documents\\UCSD 2023-2024\\dsc180a\\dsc180a-q1\\exploratory.ipynb Cell 6\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/eviye/Documents/UCSD%202023-2024/dsc180a/dsc180a-q1/exploratory.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mGenerating Word2Vec embeddings.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/eviye/Documents/UCSD%202023-2024/dsc180a/dsc180a-q1/exploratory.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m call \u001b[39m=\u001b[39m TQDMCallback(\u001b[39m5\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/eviye/Documents/UCSD%202023-2024/dsc180a/dsc180a-q1/exploratory.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m w2v \u001b[39m=\u001b[39m Word2Vec(sents, callbacks\u001b[39m=\u001b[39;49m[call]) \u001b[39m# should take ~50 sec or less\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\eviye\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\word2vec.py:430\u001b[0m, in \u001b[0;36mWord2Vec.__init__\u001b[1;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[39m=\u001b[39mcorpus_iterable, corpus_file\u001b[39m=\u001b[39mcorpus_file, passes\u001b[39m=\u001b[39m(epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))\n\u001b[0;32m    429\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuild_vocab(corpus_iterable\u001b[39m=\u001b[39mcorpus_iterable, corpus_file\u001b[39m=\u001b[39mcorpus_file, trim_rule\u001b[39m=\u001b[39mtrim_rule)\n\u001b[1;32m--> 430\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(\n\u001b[0;32m    431\u001b[0m         corpus_iterable\u001b[39m=\u001b[39;49mcorpus_iterable, corpus_file\u001b[39m=\u001b[39;49mcorpus_file, total_examples\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcorpus_count,\n\u001b[0;32m    432\u001b[0m         total_words\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcorpus_total_words, epochs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepochs, start_alpha\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49malpha,\n\u001b[0;32m    433\u001b[0m         end_alpha\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmin_alpha, compute_loss\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss, callbacks\u001b[39m=\u001b[39;49mcallbacks)\n\u001b[0;32m    434\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    435\u001b[0m     \u001b[39mif\u001b[39;00m trim_rule \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\eviye\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\word2vec.py:1073\u001b[0m, in \u001b[0;36mWord2Vec.train\u001b[1;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1070\u001b[0m     callback\u001b[39m.\u001b[39mon_epoch_begin(\u001b[39mself\u001b[39m)\n\u001b[0;32m   1072\u001b[0m \u001b[39mif\u001b[39;00m corpus_iterable \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1073\u001b[0m     trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_epoch(\n\u001b[0;32m   1074\u001b[0m         corpus_iterable, cur_epoch\u001b[39m=\u001b[39;49mcur_epoch, total_examples\u001b[39m=\u001b[39;49mtotal_examples,\n\u001b[0;32m   1075\u001b[0m         total_words\u001b[39m=\u001b[39;49mtotal_words, queue_factor\u001b[39m=\u001b[39;49mqueue_factor, report_delay\u001b[39m=\u001b[39;49mreport_delay,\n\u001b[0;32m   1076\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1077\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1078\u001b[0m     trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_epoch_corpusfile(\n\u001b[0;32m   1079\u001b[0m         corpus_file, cur_epoch\u001b[39m=\u001b[39mcur_epoch, total_examples\u001b[39m=\u001b[39mtotal_examples, total_words\u001b[39m=\u001b[39mtotal_words,\n\u001b[0;32m   1080\u001b[0m         callbacks\u001b[39m=\u001b[39mcallbacks, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\eviye\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\word2vec.py:1434\u001b[0m, in \u001b[0;36mWord2Vec._train_epoch\u001b[1;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[0;32m   1431\u001b[0m     thread\u001b[39m.\u001b[39mdaemon \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m  \u001b[39m# make interrupting the process with ctrl+c easier\u001b[39;00m\n\u001b[0;32m   1432\u001b[0m     thread\u001b[39m.\u001b[39mstart()\n\u001b[1;32m-> 1434\u001b[0m trained_word_count, raw_word_count, job_tally \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_epoch_progress(\n\u001b[0;32m   1435\u001b[0m     progress_queue, job_queue, cur_epoch\u001b[39m=\u001b[39;49mcur_epoch, total_examples\u001b[39m=\u001b[39;49mtotal_examples,\n\u001b[0;32m   1436\u001b[0m     total_words\u001b[39m=\u001b[39;49mtotal_words, report_delay\u001b[39m=\u001b[39;49mreport_delay, is_corpus_file_mode\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   1437\u001b[0m )\n\u001b[0;32m   1439\u001b[0m \u001b[39mreturn\u001b[39;00m trained_word_count, raw_word_count, job_tally\n",
      "File \u001b[1;32mc:\\Users\\eviye\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\word2vec.py:1289\u001b[0m, in \u001b[0;36mWord2Vec._log_epoch_progress\u001b[1;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[0;32m   1286\u001b[0m unfinished_worker_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers\n\u001b[0;32m   1288\u001b[0m \u001b[39mwhile\u001b[39;00m unfinished_worker_count \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 1289\u001b[0m     report \u001b[39m=\u001b[39m progress_queue\u001b[39m.\u001b[39;49mget()  \u001b[39m# blocks if workers too slow\u001b[39;00m\n\u001b[0;32m   1290\u001b[0m     \u001b[39mif\u001b[39;00m report \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# a thread reporting that it finished\u001b[39;00m\n\u001b[0;32m   1291\u001b[0m         unfinished_worker_count \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\eviye\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qsize():\n\u001b[1;32m--> 171\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnot_empty\u001b[39m.\u001b[39;49mwait()\n\u001b[0;32m    172\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    173\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be a non-negative number\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\eviye\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py:327\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    326\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 327\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[0;32m    328\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    329\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Tokenizing dataset.\")\n",
    "sents = df['sentence'].progress_apply(tokenize)\n",
    "print(\"Converting tokenized data to tuples.\")\n",
    "sents = sents.progress_apply(tuple)\n",
    "sents = sents\n",
    "print(\"Generating Word2Vec embeddings.\")\n",
    "call = TQDMCallback(5)\n",
    "w2v = Word2Vec(sents, callbacks=[call]) # should take ~50 sec or less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.save_word2vec_format('models/nyt-coarse.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = KeyedVectors.load_word2vec_format(\"models/nyt-coarse.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
